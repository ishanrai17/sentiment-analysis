{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "z9NSVTqPIZqk",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install transformers torchtext datasets fsspec==2023.6.0"
      ],
      "metadata": {
        "id": "O8dPMFxkIo51",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from datasets import load_dataset\n",
        "import gc\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "nlBfaomZJTYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSXwuXSVJoWK"
      },
      "outputs": [],
      "source": [
        "!rm -rf ~/.cache/huggingface/datasets/imdb\n",
        "!rm -rf ~/.cache/huggingface/modules"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main_dataset = load_dataset('imdb', revision=\"main\")"
      ],
      "metadata": {
        "id": "9s4xvI3rwJh4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-kkB97UGv4V"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from datasets import load_dataset\n",
        "import gc\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class MultiHeadAttention(torch.nn.Module):\n",
        "    def __init__(self, input_dim, num_heads=8):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = input_dim // num_heads\n",
        "\n",
        "        self.Q = torch.nn.Linear(input_dim, input_dim)\n",
        "        self.K = torch.nn.Linear(input_dim, input_dim)\n",
        "        self.V = torch.nn.Linear(input_dim, input_dim)\n",
        "\n",
        "        self.fc = torch.nn.Linear(input_dim, input_dim)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # Implement the forward pass\n",
        "        # x should be of shape (batch_size, seq_length, input_dim)\n",
        "        batch_size, seq_length, _ = x.shape\n",
        "        Q = self.Q(x).view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.K(x).view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.V(x).view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # new format: (batch_size, num_heads, seq_length, d_k)\n",
        "\n",
        "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (K.size(-1) ** 0.5)\n",
        "\n",
        "        if mask is not None:\n",
        "            # Apply the mask to the attention scores\n",
        "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
        "            attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        # format: (batch_size, num_heads, seq_length, seq_length)\n",
        "\n",
        "        attention_weights = torch.nn.functional.softmax(attention_scores, dim=-1)\n",
        "\n",
        "        context = torch.matmul(attention_weights, V)\n",
        "\n",
        "        # format: (batch_size, num_heads, seq_length, d_k)\n",
        "\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_length, -1)\n",
        "\n",
        "        # format: (batch_size, seq_length, input_dim)\n",
        "\n",
        "        # after concatnating the heads we run a linear layer to allow the different parts of the embeddings to interact post attention\n",
        "\n",
        "        output = self.fc(context)\n",
        "        return output\n",
        "\n",
        "\n",
        "class FeedForwardNetwork(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=128):\n",
        "        super(FeedForwardNetwork, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = torch.nn.Linear(hidden_dim, input_dim)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SentimentAnalysisModel(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=128, output_dim=1):\n",
        "        super(SentimentAnalysisModel, self).__init__()\n",
        "\n",
        "        self.multi_head_attention = MultiHeadAttention(input_dim)\n",
        "        self.norm1 = torch.nn.LayerNorm(input_dim)\n",
        "        self.dropout1 = torch.nn.Dropout(0.1)\n",
        "        self.feed_forward = FeedForwardNetwork(input_dim, hidden_dim)\n",
        "        self.norm2 = torch.nn.LayerNorm(input_dim)\n",
        "        self.dropout2 = torch.nn.Dropout(0.1)\n",
        "\n",
        "        self.multi_head_attention2 = MultiHeadAttention(input_dim)\n",
        "        self.norm3 = torch.nn.LayerNorm(input_dim)\n",
        "        self.dropout3 = torch.nn.Dropout(0.1)\n",
        "        self.feed_forward2 = FeedForwardNetwork(input_dim, hidden_dim)\n",
        "        self.norm4 = torch.nn.LayerNorm(input_dim)\n",
        "        self.dropout4 = torch.nn.Dropout(0.1)\n",
        "\n",
        "        # self.output_layer = torch.nn.Linear(input_dim, output_dim)\n",
        "        # self.dropout5 = torch.nn.Dropout(0.1)\n",
        "        # self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "        # Output layers\n",
        "        self.classifier = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_dim, hidden_dim),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(0.2),\n",
        "            torch.nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(),  lr=2e-5, weight_decay=1e-4)\n",
        "\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # x should be of shape (batch_size, seq_length, input_dim)\n",
        "        residual = x\n",
        "        x = self.multi_head_attention(x, mask)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.norm1(x + residual)\n",
        "\n",
        "        residual = x\n",
        "        x = self.feed_forward(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.norm2(x + residual)\n",
        "\n",
        "        residual = x\n",
        "        x = self.multi_head_attention2(x, mask)\n",
        "        x = self.dropout3(x)\n",
        "        x = self.norm3(x + residual)\n",
        "\n",
        "        residual = x\n",
        "        x = self.feed_forward2(x)\n",
        "        x = self.dropout4(x)\n",
        "        x = self.norm4(x + residual)\n",
        "\n",
        "        mask_expanded = mask.unsqueeze(-1).float()\n",
        "        x = x * mask_expanded\n",
        "        pooled = x.sum(dim=1) / mask.sum(dim=1, keepdim=True).clamp(min=1e-9)\n",
        "\n",
        "        # x = x.mean(dim=1)\n",
        "\n",
        "        # x = self.output_layer(pooled)\n",
        "        # x = self.dropout5(x)\n",
        "        # x = self.sigmoid(x)\n",
        "\n",
        "        x = self.classifier(pooled).squeeze(-1)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def compute_loss(self, logits, targets):\n",
        "        loss_fn =  torch.nn.BCEWithLogitsLoss()\n",
        "        targets = targets.float()\n",
        "        return loss_fn(logits.squeeze(), targets)\n",
        "\n",
        "    def train_step(self, inputs, mask, targets):\n",
        "      self.train()\n",
        "      self.optimizer.zero_grad()\n",
        "\n",
        "      # Forward pass\n",
        "      logits = self(inputs, mask)\n",
        "\n",
        "      # Compute loss\n",
        "      loss = self.compute_loss(logits, targets)\n",
        "\n",
        "      # Backward pass and optimize\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(self.parameters(), 1.0)\n",
        "      self.optimizer.step()\n",
        "\n",
        "      return loss.item()\n",
        "\n",
        "    def test_model(self, inputs, mask, targets):\n",
        "        self.eval()\n",
        "        total_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "          # Remove extra dimensions if present\n",
        "          inputs = inputs.squeeze(0) if inputs.dim() > 3 else inputs\n",
        "          mask = mask.squeeze(0) if mask.dim() > 2 else mask\n",
        "          targets = targets.squeeze(0) if targets.dim() > 1 else targets\n",
        "\n",
        "          # Forward pass\n",
        "          logits = self(inputs, mask)\n",
        "          logits = torch.sigmoid(logits)\n",
        "\n",
        "          # Compute loss\n",
        "          loss = self.compute_loss(logits, targets)\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          # Compute accuracy\n",
        "          predictions = (logits.squeeze() > 0.5).float()\n",
        "          correct_predictions += (predictions == targets.float()).sum().item()\n",
        "          total_predictions += targets.size(0)\n",
        "\n",
        "        avg_loss = total_loss / inputs.shape[0]\n",
        "        accuracy = correct_predictions / total_predictions\n",
        "\n",
        "        # print(\"Predicted: \", logits)\n",
        "        # print(\"Actual: \", targets)\n",
        "        print(avg_loss, accuracy)\n",
        "\n",
        "        return avg_loss, accuracy\n",
        "\n",
        "class Databuilder:\n",
        "    def __init__(self, batch_size=32):\n",
        "        self.dataset = main_dataset\n",
        "        self.train_data = self.dataset['train'].shuffle(seed=42)\n",
        "        self.test_data = self.dataset['test'].shuffle(seed=42)\n",
        "        self.batch_size = batch_size\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.embedder = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
        "\n",
        "        for param in self.embedder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def get_loader(self, type='train'):\n",
        "      batch_texts = []\n",
        "      batch_labels = []\n",
        "\n",
        "      loaded_data = self.dataset[type].shuffle()\n",
        "      for example in loaded_data:\n",
        "\n",
        "          batch_texts.append(example['text'])\n",
        "          batch_labels.append(int(example['label']))\n",
        "\n",
        "          if len(batch_texts) == self.batch_size:\n",
        "              yield self.vectorize_batch(batch_texts, batch_labels)\n",
        "\n",
        "              gc.collect()\n",
        "\n",
        "              batch_texts = []\n",
        "              batch_labels = []\n",
        "\n",
        "    def vectorize_batch(self, batch_texts, batch_labels):\n",
        "      tokens = self.tokenizer(batch_texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
        "\n",
        "      input_ids = tokens['input_ids'].to(device)\n",
        "      attention_mask = tokens['attention_mask'].to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "        embeddings = self.embedder(input_ids).last_hidden_state\n",
        "\n",
        "      embeddings = embeddings.detach()\n",
        "\n",
        "      labels = torch.tensor(batch_labels, dtype=torch.int8)\n",
        "\n",
        "      return (embeddings, attention_mask, labels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngiRWhEKH_cm"
      },
      "outputs": [],
      "source": [
        "print(\"Loading dataset...\")\n",
        "epochs = 10\n",
        "\n",
        "databuilder = Databuilder(batch_size=128)\n",
        "\n",
        "transformer = SentimentAnalysisModel(input_dim=768, hidden_dim=2048, output_dim=1).to(device)\n",
        "\n",
        "test_batch = next(databuilder.get_loader('test'))\n",
        "\n",
        "ti, tm, tt = test_batch\n",
        "ti = ti.to(device)\n",
        "tm = tm.to(device)\n",
        "tt = tt.to(device)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print(epoch)\n",
        "  for batch in databuilder.get_loader('train'):\n",
        "    inputs, masks, targets = batch\n",
        "\n",
        "    inputs = inputs.to(device)\n",
        "    masks = masks.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    loss = transformer.train_step(inputs, masks, targets)\n",
        "\n",
        "    loss, acc = transformer.test_model(ti, tm, tt)\n",
        "\n",
        "    # if acc >= 0.9:\n",
        "    #   break\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "print(\"Training complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleanup\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "7oAlS91WfREv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "\n",
        "test_batches = ['The movie was bad', \"I want my money back!\", \"OMG I WAS SCREAMING, IT WAS THAT GOOD!\", 'AMAZING MOVIE!']\n",
        "\n",
        "# databuilder = Databuilder(batch_size=64)\n",
        "\n",
        "# transformer = SentimentAnalysisModel(input_dim=768, hidden_dim=2048, output_dim=1).to(device)\n",
        "\n",
        "i, m, l = databuilder.vectorize_batch(test_batches, [])\n",
        "\n",
        "# print(i,m,l)\n",
        "\n",
        "print(torch.sigmoid(transformer.forward(i, m)))\n",
        "\n",
        "for text, sentiment in zip(test_batches, torch.sigmoid(transformer.forward(i, m))):\n",
        "  sentiment = \"Good\" if sentiment > 0.5 else \"Bad\"\n",
        "  print(text, \"=>\", sentiment)\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "it50Hsxd7twF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}